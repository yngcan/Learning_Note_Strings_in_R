### Chapter 7
## Practical Applications

## 7.1 Reversing a string
## 7.1.1 Reversing a string by characters
# function that reverses a string by characters
reverse_chars <- function(string)
  {
# split string by characters
string_split = strsplit(as.character(string), split = "")
# reverse order
rev_order = nchar(string):1
# reversed characters
reversed_chars = string_split[[1]][rev_order]
# collapse reversed characters
paste(reversed_chars, collapse="")
}

# try 'reverse_chars'
reverse_chars("abcdefg")
# try with non-character input
reverse_chars(12345)

# reverse vector (by characters)
lapply(c("the big bang theory", "atmosphere"), reverse_chars)

## 7.1.2 Reversing a string by words

# function that reverses a string by words
reverse_words <- function(string)
  {
# split string by blank spaces
string_split = strsplit(as.character(string), split = " ")
# how many split terms?
string_length = length(string_split[[1]])
# decide what to do
if (string_length == 1) {
# one word (do nothing)
reversed_string = string_split[[1]]
} else {
# more than one word (collapse them)
reversed_split = string_split[[1]][string_length:1]
reversed_string = paste(reversed_split, collapse = " ")
}
# output
return(reversed_string)
}

# examples
reverse_words("atmosphere")

reverse_words("the big bang theory")

# reverse vector (by words)
lapply(c("the big bang theory", "atmosphere"), reverse_words)

## 7.2 Matching e-mail addresses

# pattern
email_pat = "^([a-z0-9_\\.-]+)@([\\da-z\\.-]+)\\.([a-z\\.]{2,6})$"
# string that matches
grepl(pattern = email_pat, x = "gaston@abc.com")

# another string that matches
grep(pattern = email_pat, x = "gaston.sanchez@research-center.fr")

# unmatched email (TLD too long)
grep(pattern = email_pat, x = "gaston@abc.something")

# potential email addresses
emails = c(
  "simple@example.com",
  "johnsmith@email.gov",
  "marie.curie@college.edu",
  "very.common@example.com",
  "a.little.lengthy.but.ok@dept.example.com",
  "disposable.style.email.with+symbol@example.com",
  "not_good@email.address")
# detect pattern
str_detect(string=emails, pattern=email_pat)

## 7.3 Matching HTML elements

## 7.3.1 Getting SIG links
# read html content
mail_lists = readLines("http://www.r-project.org/mail.html")

# SIG's href pattern
sig_pattern = "^.*<td> *<a href='(https.*)'>.*$"
  
# find SIG href attributes
sig_hrefs = grep(sig_pattern, mail_lists, value = TRUE)

# let's see first 5 elements (shorten output)
shorten_sigs = rep("", 5)
for (i in 1:5) {
shorten_sigs[i] = toString(sig_hrefs[i], width=70)
}
shorten_sigs

# get first matched group
sub(sig_pattern, "\\1", sig_hrefs)

## 7.4 Text Analysis of BioMed Central Journals

# link of data set
url = "http://www.biomedcentral.com/journals/biomedcentraljournallist.txt"

# read data (stringsAsFactors=FALSE)
biomed = read.table(url, header = TRUE, sep = ",", stringsAsFactors = FALSE)

# structure of the dataset
str(biomed, vec.len = 1)

## 7.4.1 Analyzing Journal Names
# first 5 journal names
head(biomed$Journal.name, 5)

# get first 10 names
titles10 = biomed$Journal.name[1:10]
titles10

# remove punctuation
titles10 = str_replace_all(titles10, pattern = "[[:punct:]]", "")
titles10

# trim extra whitespaces
titles10 = str_replace_all(titles10, pattern = "\\s+", " ")
titles10

# remove punctuation symbols
all_titles = str_replace_all(biomed$Journal.name, pattern = "[[:punct:]]", "")
# trim extra whitespaces
all_titles = str_replace_all(all_titles, pattern = "\\s+", " ")

# split titles by words
all_titles_list = str_split(all_titles, pattern = " ")

# show first 2 elements
all_titles_list[1:10]

## Summary statistics
# how many words per title
words_per_title = sapply(all_titles_list, length)

# table of frequencies
table(words_per_title)

# distribution
100 * round(table(words_per_title)/length(words_per_title), 4)

# summary
summary(words_per_title)

# longest journal
all_titles[which(words_per_title == 9)]

##ã€€7.4.2 Common words
# vector of words in titles
title_words = unlist(all_titles_list)

# get unique words
unique_words = unique(title_words)

# how many unique words in total
num_unique_words = length(unique(title_words))
num_unique_words

# Applying unique() to the vector title 
# words we get the desired dictionary of terms.

# vector to store counts
count_words = rep(0, num_unique_words)

# count number of occurrences
for (i in 1:num_unique_words) {
count_words[i] = sum(title_words == unique_words[i])
}

# # table with word frequencies
# count_words_alt = table(title_words)

# table of frequencies
table(count_words)

# equivalently
table(count_words_alt)

# index values in decreasing order
top_30_order = order(count_words, decreasing = TRUE)[1:30]

# top 30 frequencies
top_30_freqs = sort(count_words, decreasing = TRUE)[1:30]

# select top 30 words
top_30_words = unique_words[top_30_order]
top_30_words
# barplot
barplot(top_30_freqs, border = NA, names.arg = top_30_words,
        las = 2, ylim = c(0,170))

# installing wordcloud
install.packages("wordcloud")
# load wordcloud
library(wordcloud)

# wordcloud
wordcloud(unique_words, count_words, scale=c(8,.2), min.freq=6,
          max.words=Inf, random.order=FALSE, rot.per=.15)
